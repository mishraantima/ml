

#sklearn- Lab1
# Linear Regression Model

import pandas as pd
import numpy as np

df= pd.read_csv('iris.csv')
df

	sepal.length 	sepal.width 	petal.length 	petal.width 	variety
0 	5.1 	3.5 	1.4 	0.2 	Setosa
1 	4.9 	3.0 	1.4 	0.2 	Setosa
2 	4.7 	3.2 	1.3 	0.2 	Setosa
3 	4.6 	3.1 	1.5 	0.2 	Setosa
4 	5.0 	3.6 	1.4 	0.2 	Setosa
... 	... 	... 	... 	... 	...
145 	6.7 	3.0 	5.2 	2.3 	Virginica
146 	6.3 	2.5 	5.0 	1.9 	Virginica
147 	6.5 	3.0 	5.2 	2.0 	Virginica
148 	6.2 	3.4 	5.4 	2.3 	Virginica
149 	5.9 	3.0 	5.1 	1.8 	Virginica

150 rows Ã— 5 columns

#y=ax+c
#:rain #testing   supervised ML
#80:20
df['variety'].value_counts()

Setosa        50
Versicolor    50
Virginica     50
Name: variety, dtype: int64

x=df.iloc[:,0:4].values # filter Independent variable

 

y=df.iloc[:,4].values  # filter dependent variable

#convert Y in numbers encoding

from sklearn.preprocessing import LabelEncoder

labelencoder_y= LabelEncoder()

y=labelencoder_y.fit_transform(y)

y

array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

 

 

from sklearn.model_selection import train_test_split 

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)

x_train

array([[6.5, 3. , 5.5, 1.8],
       [7. , 3.2, 4.7, 1.4],
       [6.5, 3. , 5.2, 2. ],
       [4.4, 2.9, 1.4, 0.2],
       [7.7, 3.8, 6.7, 2.2],
       [5.9, 3. , 5.1, 1.8],
       [7.7, 3. , 6.1, 2.3],
       [6.3, 2.7, 4.9, 1.8],
       [5.6, 3. , 4.5, 1.5],
       [5.2, 3.4, 1.4, 0.2],
       [5. , 3.3, 1.4, 0.2],
       [7.3, 2.9, 6.3, 1.8],
       [6.1, 2.9, 4.7, 1.4],
       [4.4, 3. , 1.3, 0.2],
       [5.9, 3. , 4.2, 1.5],
       [6. , 2.2, 4. , 1. ],
       [4.6, 3.1, 1.5, 0.2],
       [6.3, 3.3, 6. , 2.5],
       [6.4, 3.2, 4.5, 1.5],
       [5.7, 2.8, 4.5, 1.3],
       [6.9, 3.1, 4.9, 1.5],
       [5.1, 3.8, 1.6, 0.2],
       [5. , 3.5, 1.3, 0.3],
       [5.1, 3.5, 1.4, 0.2],
       [5.6, 2.7, 4.2, 1.3],
       [5.8, 2.7, 5.1, 1.9],
       [5.8, 2.8, 5.1, 2.4],
       [6.9, 3.1, 5.4, 2.1],
       [4.9, 2.5, 4.5, 1.7],
       [6.3, 2.5, 5. , 1.9],
       [5.1, 3.8, 1.9, 0.4],
       [7.7, 2.8, 6.7, 2. ],
       [5.8, 2.7, 4.1, 1. ],
       [5.5, 4.2, 1.4, 0.2],
       [6.7, 3.3, 5.7, 2.5],
       [5.7, 3.8, 1.7, 0.3],
       [6.2, 2.9, 4.3, 1.3],
       [5.9, 3.2, 4.8, 1.8],
       [6.4, 3.2, 5.3, 2.3],
       [6.3, 2.9, 5.6, 1.8],
       [4.8, 3.1, 1.6, 0.2],
       [6.4, 2.8, 5.6, 2.2],
       [6.3, 2.3, 4.4, 1.3],
       [7.2, 3.2, 6. , 1.8],
       [6.9, 3.2, 5.7, 2.3],
       [4.7, 3.2, 1.6, 0.2],
       [7.4, 2.8, 6.1, 1.9],
       [6.7, 3.1, 4.7, 1.5],
       [4.6, 3.6, 1. , 0.2],
       [6.4, 2.8, 5.6, 2.1],
       [6.7, 3.3, 5.7, 2.1],
       [4.9, 3.1, 1.5, 0.2],
       [6.1, 3. , 4.6, 1.4],
       [5.5, 2.4, 3.8, 1.1],
       [5.4, 3.9, 1.3, 0.4],
       [5.2, 3.5, 1.5, 0.2],
       [4.8, 3.4, 1.6, 0.2],
       [6.1, 2.8, 4. , 1.3],
       [4.4, 3.2, 1.3, 0.2],
       [6.6, 2.9, 4.6, 1.3],
       [6.8, 3.2, 5.9, 2.3],
       [4.6, 3.4, 1.4, 0.3],
       [7.1, 3. , 5.9, 2.1],
       [5.1, 2.5, 3. , 1.1],
       [6.2, 3.4, 5.4, 2.3],
       [6.8, 2.8, 4.8, 1.4],
       [5. , 3. , 1.6, 0.2],
       [6.1, 3. , 4.9, 1.8],
       [4.9, 3.6, 1.4, 0.1],
       [5.6, 2.5, 3.9, 1.1],
       [6. , 2.2, 5. , 1.5],
       [4.3, 3. , 1.1, 0.1],
       [5.5, 2.5, 4. , 1.3],
       [6.2, 2.2, 4.5, 1.5],
       [5. , 2.3, 3.3, 1. ],
       [5.3, 3.7, 1.5, 0.2],
       [5.1, 3.7, 1.5, 0.4],
       [6. , 3. , 4.8, 1.8],
       [6.3, 3.4, 5.6, 2.4],
       [5.4, 3.7, 1.5, 0.2],
       [5.8, 2.6, 4. , 1.2],
       [5.6, 2.8, 4.9, 2. ],
       [6.7, 3. , 5. , 1.7],
       [6.4, 2.9, 4.3, 1.3],
       [6. , 2.7, 5.1, 1.6],
       [7.7, 2.6, 6.9, 2.3],
       [5.8, 2.7, 3.9, 1.2],
       [6.2, 2.8, 4.8, 1.8],
       [5.6, 3. , 4.1, 1.3],
       [5.8, 4. , 1.2, 0.2],
       [6.1, 2.6, 5.6, 1.4],
       [6.7, 3.1, 4.4, 1.4],
       [5.5, 2.6, 4.4, 1.2],
       [6.4, 2.7, 5.3, 1.9],
       [5.7, 2.9, 4.2, 1.3],
       [5.7, 2.5, 5. , 2. ],
       [5.5, 2.4, 3.7, 1. ],
       [4.8, 3. , 1.4, 0.1],
       [5.1, 3.4, 1.5, 0.2],
       [5.2, 4.1, 1.5, 0.1],
       [4.6, 3.2, 1.4, 0.2],
       [6.6, 3. , 4.4, 1.4],
       [5. , 3.4, 1.6, 0.4],
       [4.7, 3.2, 1.3, 0.2],
       [5. , 2. , 3.5, 1. ]])

x_train.size

420

x_test.size

180

y_train.size

105

y_test.size

45

from sklearn.linear_model import LogisticRegression

logmodel=LogisticRegression()
logmodel.fit(x_train,y_train)

/home/administrato/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

LogisticRegression()

y_pred= logmodel.predict(x_test)

y_pred
np.sort(y_pred)

array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2])

y_test
np.sort(y_test)

array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2])

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test,y_pred)

array([[17,  0,  0],
       [ 0, 14,  0],
       [ 0,  0, 14]])

(28/30)*100

93.33333333333333

logmodel.score(x_test,y_test)

1.0

from sklearn.neighbors import KNeighborsClassifier  

classifier_knn= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2) # distance name

classifier_knn.fit(x_train,y_train) # model training 70:30

KNeighborsClassifier()

y_pred_knn=classifier_knn.predict(x_test)

y_pred_knn

array([1, 2, 0, 1, 1, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 1, 0, 2, 2, 0, 2,
       0, 1, 2, 0, 2, 2, 1, 1, 1, 0, 2, 0, 0, 2, 1, 1, 1, 0, 1, 0, 1, 2,
       0])

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test,y_pred_knn)

array([[17,  0,  0],
       [ 0, 13,  1],
       [ 0,  0, 14]])

classifier_knn.score(x_test,y_test)

0.9777777777777777

 

from sklearn.metrics import accuracy_score
accuracy_score(y_pred_knn,y_test)

0.9777777777777777

# NaiveBias Model

from sklearn.naive_bayes import GaussianNB

classifier_nb= GaussianNB()

 

classifier_nb.fit(x_train,y_train)

GaussianNB()

y_pred= classifier_nb.predict(x_test)

y_pred

array([1, 2, 0, 1, 1, 0, 2, 2, 0, 2, 0, 0, 0, 1, 0, 2, 1, 0, 1, 2, 0, 2,
       0, 1, 2, 0, 2, 2, 1, 1, 1, 0, 2, 0, 0, 2, 1, 1, 1, 0, 1, 0, 1, 2,
       0])

from sklearn.metrics import accuracy_score
accuracy_score(y_pred_knn,y_test)

0.9777777777777777

confusion_matrix(y_test,y_pred)

array([[17,  0,  0],
       [ 0, 14,  0],
       [ 0,  1, 13]])

classifier_nb.score(x_test,y_test)

0.9777777777777777

 

 

